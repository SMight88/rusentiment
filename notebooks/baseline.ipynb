{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize, TweetTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "import vecto.embeddings\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/')\n",
    "DATASET_DIR = DATA_DIR.joinpath('dataset/')\n",
    "FASTTEXT_DIR = DATA_DIR.joinpath('embeddings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = pd.read_csv(PATH_TO_DATASET + 'rusentiment_random_posts.csv')\n",
    "df_preselected = pd.read_csv(PATH_TO_DATASET + 'rusentiment_preselected_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallDeepNet(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, nb_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.nb_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.classifier(inputs)\n",
    "        outputs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, y_pred, target):\n",
    "        loss = self.criterion(y_pred, target)\n",
    "\n",
    "        # Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        self.optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    try:\n",
    "        embeddings = vecto.embeddings.load_from_dir(filename)\n",
    "\n",
    "    except EOFError:\n",
    "        print(f'Cannot load: {filename}')\n",
    "        embeddings = None\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_matrix_embeddings(samples, word_embeddings):\n",
    "    embeddings_dim = len(word_embeddings.matrix[0])\n",
    "    nb_samples = len(samples)\n",
    "    X = np.zeros((nb_samples, embeddings_dim), dtype=np.float32)\n",
    "\n",
    "    nb_empty = 0\n",
    "    empty_samples = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        tokens = sample.split(' ')\n",
    "        tokens_embeddings = [word_embeddings.get_vector(t) for t in tokens if\n",
    "                             word_embeddings.has_word(t)]\n",
    "        if len(tokens_embeddings) > 0:\n",
    "            mean_embeddings = np.mean(tokens_embeddings, axis=0)\n",
    "            X[i] = mean_embeddings\n",
    "        else:\n",
    "            nb_empty += 1\n",
    "            empty_samples.append(tokens)\n",
    "\n",
    "    print(f'Empty samples: {nb_empty}')\n",
    "\n",
    "    return X, empty_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        samples = []\n",
    "        labels = []\n",
    "        for row in reader:\n",
    "            text = row['text']\n",
    "            label = row['label']\n",
    "\n",
    "            text_tokenized = tokenizer.tokenize(text)\n",
    "\n",
    "            text_joined = ' '.join(text_tokenized)\n",
    "\n",
    "            samples.append(text_joined)\n",
    "            labels.append(label)\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(mode, labels_mode):\n",
    "    data_base_filename = DATASET_DIR.joinpath('rusentiment_random_posts.csv')\n",
    "    data_posneg_filename = DATASET_DIR.joinpath('rusentiment_preselected_posts.csv')\n",
    "    data_test_filename = DATASET_DIR.joinpath('rusentiment_test.csv')\n",
    "\n",
    "    samples_base_train, labels_base_train = load_data(data_base_filename)\n",
    "    samples_posneg_train, labels_posneg_train = load_data(data_posneg_filename)\n",
    "    samples_test, labels_test = load_data(data_test_filename)\n",
    "\n",
    "    print(f'Data base: {len(samples_base_train)}, {len(labels_base_train)}')\n",
    "    print(f'Data posneg: {len(samples_posneg_train)},'\n",
    "          f' {len(labels_posneg_train)}')\n",
    "    print(f'Data test: {len(samples_test)}, {len(labels_test)}')\n",
    "    print(f'Labels: {len(set(labels_base_train))},'\n",
    "          f' {len(set(labels_base_train))}, {len(set(labels_test))}')\n",
    "\n",
    "    if mode == 'base':\n",
    "        samples_train = samples_base_train\n",
    "        labels_train = labels_base_train\n",
    "    elif mode == 'posneg':\n",
    "        samples_train = samples_base_train + samples_posneg_train\n",
    "        labels_train = labels_base_train + labels_posneg_train\n",
    "    elif mode == 'pos':\n",
    "        target_class = 'positive'\n",
    "        target_samples = \\\n",
    "            [s for s, l in zip(samples_posneg_train, labels_posneg_train)\n",
    "             if l == target_class]\n",
    "        target_labels = [target_class] * len(target_samples)\n",
    "        samples_train = samples_base_train + target_samples\n",
    "        labels_train = labels_base_train + target_labels\n",
    "    elif mode == 'neg':\n",
    "        target_class = 'negative'\n",
    "        target_samples = \\\n",
    "            [s for s, l in zip(samples_posneg_train, labels_posneg_train)\n",
    "             if l == target_class]\n",
    "        target_labels = [target_class] * len(target_samples)\n",
    "        samples_train = samples_base_train + target_samples\n",
    "        labels_train = labels_base_train + target_labels\n",
    "    elif mode == 'neutral':\n",
    "        target_class = 'neutral'\n",
    "        target_samples = \\\n",
    "            [s for s, l in zip(samples_posneg_train, labels_posneg_train)\n",
    "             if l == target_class]\n",
    "        target_labels = [target_class] * len(target_samples)\n",
    "        samples_train = samples_base_train + target_samples\n",
    "        labels_train = labels_base_train + target_labels\n",
    "    elif mode == 'posneg_only':\n",
    "        samples_train = samples_posneg_train\n",
    "        labels_train = labels_posneg_train\n",
    "    elif mode == 'replace':\n",
    "        nb_replace = len(samples_posneg_train)\n",
    "        samples_base_train, labels_base_train = \\\n",
    "            shuffle(samples_base_train, labels_base_train)\n",
    "        samples_train = samples_base_train[:-nb_replace] + samples_posneg_train\n",
    "        labels_train = labels_base_train[:-nb_replace] + labels_posneg_train\n",
    "    elif mode == 'debug':\n",
    "        nb_samples_debug = 2000\n",
    "        samples_train = samples_base_train[:nb_samples_debug]\n",
    "        labels_train = labels_base_train[:nb_samples_debug]\n",
    "    elif mode == 'sample':\n",
    "        nb_sample = len(samples_posneg_train)\n",
    "        samples_base_train, labels_base_train = shuffle(\n",
    "            samples_base_train, labels_base_train)\n",
    "        samples_train = samples_base_train[:nb_sample]\n",
    "        labels_train = labels_base_train[:nb_sample]\n",
    "    elif mode == 'sample_posneg':\n",
    "        nb_samples_by_classes = Counter(labels_posneg_train)\n",
    "\n",
    "        samples_train = []\n",
    "        labels_train = []\n",
    "        for target_class, target_counts in nb_samples_by_classes.most_common():\n",
    "            base_samples_of_target_class = [\n",
    "                s for s, l in zip(samples_base_train, labels_base_train)\n",
    "                if l == target_class]\n",
    "            shuffle(base_samples_of_target_class)\n",
    "            base_samples_of_target_class = \\\n",
    "                base_samples_of_target_class[:target_counts]\n",
    "\n",
    "            samples_train.extend(base_samples_of_target_class)\n",
    "            labels_train.extend([target_class] * len(base_samples_of_target_class))\n",
    "    else:\n",
    "        raise ValueError(f'Mode {mode} is unknown')\n",
    "\n",
    "    if labels_mode == 'base':\n",
    "        pass\n",
    "    elif labels_mode == 'neg':\n",
    "        labels_train = ['rest' if lbl != 'negative' else lbl for lbl in labels_train]\n",
    "        labels_test = ['rest' if lbl != 'negative' else lbl for lbl in labels_test]\n",
    "    elif labels_mode == 'pos':\n",
    "        labels_train = ['rest' if lbl != 'positive' else lbl for lbl in labels_train]\n",
    "        labels_test = ['rest' if lbl != 'positive' else lbl for lbl in labels_test]\n",
    "    else:\n",
    "        raise ValueError(f'Labels mode {labels_mode} is unknown')\n",
    "\n",
    "    return samples_train, labels_train, samples_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, X, y_true, labels):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    if len(set(y_true)) == 2:\n",
    "        average = 'binary'\n",
    "        pos_label = int(np.argwhere(labels != 'rest'))\n",
    "    else:\n",
    "        average = 'weighted'\n",
    "        pos_label = 1\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, pos_label=pos_label)\n",
    "    precision = precision_score(y_true, y_pred, average=average, pos_label=pos_label)\n",
    "    recall = recall_score(y_true, y_pred, average=average, pos_label=pos_label)\n",
    "\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data base: 21268, 21268\n",
      "Data posneg: 6950, 6950\n",
      "Data test: 2967, 2967\n",
      "Labels: 5, 5, 5\n"
     ]
    }
   ],
   "source": [
    "samples_train, labels_train, samples_test, labels_test = create_training_data(mode='posneg',\n",
    "                                                                              labels_mode='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28218"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data train: 28218\n",
      "Labels train: Counter({'neutral': 11300, 'positive': 6110, 'skip': 4094, 'negative': 3654, 'speech': 3060})\n"
     ]
    }
   ],
   "source": [
    "print(f'Data train: {len(samples_train)}')\n",
    "print(f'Labels train: {Counter(labels_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 507470\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_embeddings(str(FASTTEXT_DIR))\n",
    "print(f'Word embeddings: {len(embeddings.vocabulary.lst_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['negative' 'neutral' 'positive' 'skip' 'speech']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels_train)\n",
    "print(f'Labels: {label_encoder.classes_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty samples: 191\n",
      "Train data: (28218, 300), (28218,)\n"
     ]
    }
   ],
   "source": [
    "X_train, empty_samples = create_data_matrix_embeddings(samples_train, embeddings)\n",
    "y_train = label_encoder.transform(labels_train)\n",
    "print(f'Train data: {X_train.shape}, {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty samples: 19\n",
      "Test data: (2967, 300), (2967,)\n"
     ]
    }
   ],
   "source": [
    "X_test, empty_samples_test = create_data_matrix_embeddings(samples_test, embeddings)\n",
    "y_test = label_encoder.transform(labels_test)\n",
    "print(f'Test data: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== RESULTS =====\n",
      "LogisticRegression: F1 train 0.631\n",
      "LinearSVC: F1 train 0.623\n",
      "GradientBoostingClassifier: F1 train 0.680\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    GradientBoostingClassifier(),\n",
    "    # net,\n",
    "]\n",
    "results = []\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)  # , sample_weight=sample_weight\n",
    "\n",
    "    result = score_model(model, X_train, y_train, label_encoder.classes_)\n",
    "    results.append(result)\n",
    "\n",
    "print('===== RESULTS =====')\n",
    "for model, (accuracy_train, f1_train, precision_train, recall_train) in zip(models, results):\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f'{model_name}: F1 train {f1_train:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== RESULTS TEST =====\n",
      "LogisticRegression: F1 test 0.688\n",
      "LinearSVC: F1 test 0.674\n",
      "GradientBoostingClassifier: F1 test 0.687\n"
     ]
    }
   ],
   "source": [
    "results_test = []\n",
    "for model in models:\n",
    "\n",
    "    result = score_model(model, X_test, y_test, label_encoder.classes_)\n",
    "    results_test.append(result)\n",
    "\n",
    "print('===== RESULTS TEST =====')\n",
    "for model, (accuracy_train, f1_train, precision_train, recall_train) in zip(models, results_test):\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f'{model_name}: F1 test {f1_train:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
